---
layout: post
title: 阅读cvpr18的论文
---
看不进去了。。下次再看 Joint Optimization Framework for Learning with Noisy Labels, between class learning for image classification, finding beans in burgers: deep semantic-visual embedding with localization, mix and match networks encoder-decoder alignment for zero-pair image translation

## Convolutional Image Captioning
把pixel cnn 里的mask convolution用在文本上了。[代码(pytorch)](https://github.com/aditya12agd5/convcap)

## Learning to Evaluate Image Captioning
没看论文。。好像是对于每个算法生成的caption，都训练一个分类器分类它生成的caption和人类标注的caption。

## [deflection adversarial attacks with pixel deflection]
没细看。。好像说的是分类器对噪声不敏感，但是adversarial attack敏感，并且adversarial attack造成的CAM的空间分布更均匀。

## [Multi-Agent Diverse Generative Adversarial Networks](http://openaccess.thecvf.com/content_cvpr_2018/papers/Ghosh_Multi-Agent_Diverse_Generative_CVPR_2018_paper.pdf)
没细看，好像是用了多个g，d在判断生成还是真实的同时还要判断是那个g生成的。

## [Customized Image Narrative Generation via Interactive Visual Question Generation and Answering](http://openaccess.thecvf.com/content_cvpr_2018/papers/Shin_Customized_Image_Narrative_CVPR_2018_paper.pdf)
通过重复生成问题和回答产生长的说明。懒得细看了。

![poster](https://preview.ibb.co/bUyN28/206_E6_DFE_A922_411_E_8_A51_B66761_E9_D554.png)

## [Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present](http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Regularizing_RNNs_for_CVPR_2018_paper.pdf)
用另一个RNN预测一个RNN的前一个hidden state作为regularization。没细看论文。

![poster](https://image.ibb.co/jtQ228/385520_A7_3506_41_BB_B10_A_FDEE3_E981_FEF.png)

## [Neural Style Transfer via Meta Networks](http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Neural_Style_Transfer_CVPR_2018_paper.pdf)
输入风格图片，输出迁移成这个风格的CNN的参数。

## [Deep Image Prior](http://openaccess.thecvf.com/content_cvpr_2018/papers/Ulyanov_Deep_Image_Prior_CVPR_2018_paper.pdf)
这篇的海报被包围得都看不到了，很好奇为什么这么受欢迎。回来看论文。这篇文章发现CNN本身就是很好的图片先验，不用训练也可以实现去噪、超分辨率之类的操作。

### 去噪
把一个均匀分布的随机张量输入CNN，训练最小化它的输出和有噪声图片之间的L2距离。随着训练次数的增加，CNN的输出会先拟合图片，训练次数很大之后才会拟合噪声。所以用小次数能实现去噪。

### 超分辨率
把大尺寸随机张量输入CNN，训练最小化它的输出经过下采样之后和小图片之间的L2距离。

### Inpainting
对于原有部分的像素，最小化输出和原图的L2

## [ST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing](http://openaccess.thecvf.com/content_cvpr_2018/papers/Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper.pdf)
把物体作空间变换了之后安在背景上。３月份的时候我尝试过这种做法，当时直接让generator输出空间变换参数，一次成型地变换，用的是原始GAN的损失函数，结果没弄出来。他们的做法是训练好几个generator，第一个generator根据原始的物体和背景输出第一次变换的参数，其他的g是根据上一次变换之后的物体和背景输出参数的残差、加上上一个g输出的参数，作为这一步变换的参数。训练时是先训练第一个g，然后固定它的参数而训练下一个，直到最后一个。最后再把整个体系一起微调。另外他们的损失函数是WGAN的，还加上了变换参数的L2范数正规化项。

有[代码 (tensorflow)](https://github.com/chenhsuanlin/spatial-transformer-GAN)

![poster](https://image.ibb.co/jDdph8/84_D5_BFDE_0150_4228_AFD7_4_EAD19_FAB708.png)

## [Low-Shot Learning from Imaginary Data](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Low-Shot_Learning_From_CVPR_2018_paper.pdf)
一种可以提高用meta-learning做few-shot learning的效果的插件。

### 基于meta-learning的few-shot learning
meta-train数据的每个类都有大量训练样本和测试样本。meta-test数据的类别是meta-train里没有的，而且每个类都只有少量训练样本。目的是利用meta-train数据训练一个从【少量训练数据】到【分类器】的映射，称为meta-learner，然后把这个映射作用在meta-test数据里的训练样本上，得到新类别的分类器。

meta-learner的输入是few-shot训练样本，输出是一个分类器。用于训练meta-learner的损失是分类器给meta-train数据里的测试样本进行分类的误差。

### 动机
Many modes of variation are shared accross categories. 可以利用这些变化生成新物体在不同的姿势和环境的新样本。用于增加样本数量。但是他们不是学习不同的姿态而是通过meta-learning产生useful for learning classifiers的生成样本。

### 做法
生成器的输入是随机向量和一个真实样本，然后用生成的样本和真实的样本一起作为meta-learner的输入，产生分类器。再用分类器在meta-train数据里的测试数据上的分类误差更新生成器。这要求meta-learner产生的分类器的输出对输入的样本可导。比如prototypical nets, match nets就满足这个要求。

## [Revisiting Dilated Convolution: A Simple Approach for Weakly- and SemiSupervised Semantic Segmentation](http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0812.pdf)
用image-level的标签产生的分割结果只能高亮出目标的discriminative部分。这个文章通过使用扩张卷积高亮目标的更大区域。用vgg产生的特征图经过扩张率为1, 3, 6, 9的扩张卷积，再经过全局平均池化和全连接，产生分类结果，用图像级别的标签训练。【论文里没有说，不过我猜这里是4个扩张率的结果分别经过全连接层、分别和真值计算损失】

为了产生分割结果，把全局平均池化去掉，用CAM的方法分别用扩张率不同的卷积产生4个分割结果。然后把它们平均，再和扩张率为1的结果相加，用Selfexplanatory deep salient object detectionSelfexplanatory deep salient object detection产生背景区域，再用和Adversarial erasing里同样的方法合并分割结果和背景区域。

## [Few-Shot Image Recognition by Predicting Parameters from Activations](http://openaccess.thecvf.com/content_cvpr_2018/papers/Qiao_Few-Shot_Image_Recognition_CVPR_2018_paper.pdf)
CNN的最后一层的特征，样本的activation和全连接层的参数向量作内积，内积最大的那个类别对应输出类别。直观地想，一个类别对应的全连接层的向量应该和这个类的所有样本的activation都有较大内积，即在样本的activation的均值附近。既然最后一个全连接层某个类别的参数和这个类别样本的activation有关，就有可能建立一个从【一个类的几个样本的activation的均值】到【全连接层里这个类的参数】的映射。

## [Neural Baby Talk](http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0205.pdf)
因为检测的结果现在已经很准确了。根据目标检测的结果和句子模板生成说明。懒得细看了

## [Context Encoding for Semantic Segmentation](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Context_Encoding_for_CVPR_2018_paper.pdf)
把传统方法用在深度学习体系中。把[Deep TEN: Texture Encoding Network](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Deep_TEN_Texture_CVPR_2017_paper.pdf)用在语义分割上了，还加上了个image-level的loss：预测图片里出现了的类别。

C*H*W的特征图作为H*W个C维向量$\{x_i\}$。另外还有K个C维向量$\{c_k\}$作为codebook（我猜直接是参数了。要从代码得知。[代码 (pytorch)](https://github.com/zhanghang1989/PyTorch-Encoding)）。用这些codeword的加权和表示每个$x_i$，计算残差$e_{ik} = x_i - \sum_k a_{ik} c_k$。权重是残差的softmax。然后把每个codeword的残差和作为特征、进行后续操作。

## [Cross-modal Deep Variational Hand Pose Estimation](http://openaccess.thecvf.com/content_cvpr_2018/papers/Spurr_Cross-Modal_Deep_Variational_CVPR_2018_paper.pdf)
把3d姿势和rgb图片映射到同一个隐变量空间。做法和vae基本上一样

## [Conditional Image-to-Image Translation](https://arxiv.org/pdf/1805.00251.pdf)

## [Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/1803.10464)

## [Generalized Zero-Shot Learning via Synthesized Examples](https://arxiv.org/pdf/1712.03878.pdf)
用VAE生成样本做0样本学习。

## [Learning to Segment Every Thing](https://arxiv.org/pdf/1711.10370.pdf)
用模型根据产生方框的参数输出产生mask的参数。这样就可以对于那些只有方框annotation的东西也给出分割的预测了。

## [Unsupervised Feature Learning via Non-Parametric Instance Discrimination](https://arxiv.org/pdf/1805.01978.pdf)
把每个图片当成一个类，训练神经网络给图片分类以学习特征。好像分类的结果还不错。看起来有些难以置信。。。作者给了[代码(pytorch)](https://github.com/zhirongw/lemniscate.pytorch)。

## [Relation Networks for Object Detection](https://arxiv.org/pdf/1711.11575.pdf)
提出了一套方法描述方框之间的相似性。把所有方框的特征的加权和作为每个方框的特征。权重根据两个方框的特征的相似性得出。相似性的计算包括方框坐标和框内特征。

## [What do Deep Networks Like to See?](https://arxiv.org/pdf/1803.08337.pdf)
把一个训练好的自编码器的编码器固定，把一个训练好的分类网络固定，用分类的损失更新自编码器里的解码器。这样就能训练自编码器让有利于分类正确的部分通过并去除输入图片里的噪声。

发现用不同分类网络训练出来的解码器图像都不一样。VGG的解码器图像和原图最接近，AlexNet和ResNet都带来了棋盘格子效应。Inception的解码器图片带有粉色调。

用分类网络训练出来的解码器有去噪作用。给图片加上均匀分布的噪声，比较直接用加噪图片分类、用加噪图片经过预训练解码器分类、加噪图片用分类训练解码器处理再分类，发现用分类训练的解码器处理噪声图片之后分类的精度对噪声最不敏感。

交换解码器的实验。比如说VGG的解码器给ResNet用这种，可以比较不同的分类网络利用的信息量大小。

## Free Supervision From Video Games
想看来着没找到pdf

## Adversarial Metric Learning
上网搜了一下只有篇[IJCAI的标题一模一样的论文](https://arxiv.org/abs/1802.03170)。等CVPR的出来了再看看

## [Decoupled Networks](https://arxiv.org/pdf/1804.08071.pdf)
作者发现CNN的特征本身是decoupled的，幅度表示intra-class variation，角度表示semantic difference。提出把这种关系显示表达：图像块$\textbf{x}$和kernel $\textbf{w}$的内积$<\textbf{x}, \textbf{w}>$可以写成$||\textbf{w}||_2 ||\textbf{x}||_2 \cos (\theta(\textbf{w}, \textbf{x}))$。根据这个把现有CNN的内积操作换成关于幅度的函数$h(||\textbf{w}||, ||\textbf{x}||)$和关于角度的函数$g(\theta(\textbf{w}, \textbf{x}))$的乘积。

看起来有意思。[作者主页](http://wyliu.com/)给了代码链接但是打不开

## [Excitation Backprop for RNNs](https://arxiv.org/pdf/1711.06778.pdf)
好像是把CNN的exitation backprop扩展到RNN了。先看看CNN的。

### [Top-down Neural Attention by Excitation Backprop](https://arxiv.org/pdf/1608.00507.pdf)
解决的问题：一个CNN的最后一个全连接层输出了某个分类结果（即某个类的得分最高），那么从第一层到最后一层这一路上都是那些神经元被激活了，导致最后的分类结果？

设$C_j$是和$a_i$相连的浅一层的所有神经元组成的集合。把P(a_j激活|a_i激活)定义为eqn.4。那么就有$P(a_j) = \sum_{i} P(a_j|a_i) P(a_i)$。

懒得写了。。

## [	Bootstrapping the Performance of Webly Supervised Semantic Segmentation]
这个也打算看的但没找到pdf

## [	Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features]
打算看的，结果没找到pdf

## [Adversarial Complementary Learning for Weakly Supervised Object Localization](https://arxiv.org/pdf/1804.06962.pdf)
对CAM和AE的改进。就是把AE变成并行的了。首先用分类器A得到class activation maps，然后把特征图里对应A的class activation maps值比较大的像素置0，输入分类器B得到互补的class activation maps，然后两个取最大值得到最终的class activation maps。

### [复习CAM (Class Activation Mapping](https://arxiv.org/pdf/1512.04150.pdf)
一个CNN最后一个卷积层输出的长H宽H通道K的特征图$S$经过平均池化，得到一个K维的向量$\textbf{s}$。用这个K维向量和一个KxC的矩阵相乘得到每个类的得分。即第c类的得分$y_c = \sum_{i=1}^K s_i W_{ic}$。注意到这个向量$\textbf{s}$是由特征图平均池化得到的。如果不对特征图池化，而是直接进行加权和$Y_c = \sum_{i=1}^K S_i W_{ic}$，就能得到第c类的activation map了。

### [STC](http://users.eecs.northwestern.edu/~xsh835/assets/pami_stc.pdf)
读这篇时发现他们的一个参考文献。。用有image level的标签的图片加上显著性检测，产生语义分割的标签。我前一阵本想这么做的，又发现已经有人这么做过了。。。弱监督分割仿佛被Yunchao Wei承包了。。。

## [Image Generation from Scene Graphs](https://arxiv.org/pdf/1804.01622.pdf)
用节点表示场景里的物体，边表示物体之间的关系。首先用一个神经网络处理场景图（大概是把边和节点都one-hot编码的场景图吧），得到每个节点和边的向量。然后把这个向量输入方框回归网络和产生物体mask的网络。然后把物体的mask填到方框里，得到场景布局（即语义分割的真值那种样子）。最后用Cascaded Refinement Netword根据场景布局生成图片。

## [Compare and Contrast: Learning Prominent Visual Differences](https://arxiv.org/pdf/1804.00112.pdf)
输出并比较两个图片主要的不同。没仔细看

## [Hierarchical Novelty Detection for Visual Object Recognition](https://arxiv.org/pdf/1804.00722.pdf)
利用层次分类学，让模型对未见过的样本给出更informative的分类结果，而不是仅仅分类成没见过的类。比如训练时没见过安哥拉猫，测试时能把安哥拉猫分类成没见过的猫，而不仅仅是没见过的类。这篇文章提供了两个模型，一个top-down和一个flatten模型。

**top-down**

为“以超类为条件，子类的条件概率$P(y|x, s; \theta_s)$”建模。对于一个超类$s$，如果以它为条件的各个子类的分布和均匀分布的KL距离大于阈值$\lambda_s$，就把其中概率最大的子类作为下一个超类。然后再以这个新的超类为条件，继续计算条件概率$P(y|x, s;\theta_s)$。直到终于出现了均匀分布的条件概率、或者到了一个叶子节点。为了使得模型能在unconfident的时候给出接近均匀分布的预测，损失函数除了要最大化以正确超类$s$为条件的子类分类的概率$P(y|x,s;\theta_s)$之外，还要最小化以错误超类$\mathcal{O}(s)$为条件的子类分类的概率和均匀分布之间的KL距离。（论文里的eqn.(1)）

**flatten**: 

为每个超类增加一个子节点作为新类，比如，猫的子节点有波斯猫和暹罗猫，增加子节点之后，猫的子节点就有波斯猫、暹罗猫和新猫。训练时，对于一个类的样本，依次把它从每个层次移除作为那个层次的新类。比如说，第一阶段把波斯猫移除作为新猫，第二阶段把所有猫移除作为新动物。【这里没有看得很懂。训练时到底移除哪个类呢？从eqn.(3)看，应该是每个类都会被移除，然后对所有类取数学期望。具体到实践上如何操作呢？作者说在附录里，太复杂了懒得看了】

## [Imagine it for me: Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts](https://arxiv.org/pdf/1712.01381.pdf)
把gan应用在0样本学习。用gan根据维基百科的文字描述产生未知样本的feature、把0样本学习转化成了全监督学习。

## [Dynamic-structured Semantic Propagation Network](https://arxiv.org/pdf/1803.06067.pdf)
现有的模型没有考虑类之间的semantic hierarchy。比如，长颈鹿/斑马/马就有很多明显区别于猫/狗的特点。这篇的模型，训练时按照类的分层建立动态的计算图。

## [The Unreasonable Effectiveness of Deep Features as a Perceptual Metric](https://arxiv.org/pdf/1801.03924.pdf)
他们发现用CNN的特征计算的图片感知距离，无论用的是unsupervised, self-supervsied, supervsied的，都很符合人类的感知（与传统的L2, SSIM, PSNR相比）。

## [Learning Superpixels with Segmentation-Aware Affinity Loss](https://sites.google.com/site/wctu1009/cvpr18_superpixel)
有一丝兴趣，但没搜到全文，等有全文了再看看

## [Scale-Transferrable Object Detection](https://pan.baidu.com/s/1i6Yjvpz)
是我没看懂吗？貌似就是在densenet的feature上用了pixel-suffle上采样，还给pixel-suffle取名为scale transferrable module。。。。

## [Disentangled Person Image Generation](https://arxiv.org/pdf/1712.02621.pdf)
人图片的三个因素是姿势、前景、背景。首先，理清输入图像的这三个因素并用特征来编码【即把人图片的pose, 前景/背景mask分别映射成向量】，并用这编码的特征重建图像。把高斯噪声映射到这种特征，产生新图片。【似乎就是vae和gan的结合，特别针对人图片的】

## [ICLR18有个用gan做one-shot learning的](https://openreview.net/pdf?id=S1Auv-WRZ)
懒得看了，下次再看

## [one-class classification](https://arxiv.org/pdf/1802.09088.pdf)
用GAN做D区分R(x)和x，然后D作为那个新样本检测器

## [一个新的vqa数据集](https://arxiv.org/abs/1801.08163)

## [Multi-Agent Diverse Generative Adversarial Networks](https://arxiv.org/abs/1704.02906)

## [Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning](https://arxiv.org/abs/1802.09129)
上次组会有人讲了，懒得看了

## [self-contradiction什么的](https://arxiv.org/abs/1604.05132)

## [Learning to Adapt Structured Output Space for Semantic Segmentation](https://arxiv.org/abs/1802.10349)。
用gan做domian adaptationa，看起来有点意思，[gta5dataset](https://download.visinf.tu-darmstadt.de/data/from_games/index.html)

用游戏生成的训练数据(source domian)训练分割模型。因为游戏数据和真实数据的target是相似的，所以在真实数据上测试时用游戏数据的标签作domain adaptation。具体：用D分类预测的标签和游戏数据的标签。

## [semi parametric](http://vladlen.info/papers/SIMS.pdf)
此篇引用了我那个只得了70分的本科毕设:)

输入语义分割，输出图片怎么做：对于语义分割里的每个区域，在训练集里检索一个最类似的图片区域，经过训练仿射变换和裁剪，拼凑在一起。（仿射变换和裁剪的模型（模型1）是预训练的）然后再去除拼接的边缘。把拼接的结果输入一个模型，训练这个模型（模型2）弥补边缘。用生成图片和真实图片的感知距离（VGGfeature的L1）距离训练模型2。

## [Visual Dialog with Discriminative Question Generation and Answering](https://arxiv.org/pdf/1803.11186.pdf)

## [vqa memory argumented](https://arxiv.org/abs/1707.04968)

## [single shot finement](https://arxiv.org/abs/1711.06897)

## [dynamic zoom in 大图片](https://arxiv.org/abs/1711.05187)

## [vqa和vqg的对偶](http://cvboy.com/pdf/publications/cvpr2018_iqan.pdf)
本来打算当做开题答辩的思路，突然发现已经被大佬们做过了，，，
