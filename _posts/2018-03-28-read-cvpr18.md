---
layout: post
title: 阅读cvpr18的论文
---
## [Unsupervised Feature Learning via Non-Parametric Instance Discrimination](https://arxiv.org/pdf/1805.01978.pdf)
把每个图片当成一个类，训练神经网络给图片分类以学习特征。好像分类的结果还不错。看起来有些难以置信。。。作者给了[代码(pytorch)](https://github.com/zhirongw/lemniscate.pytorch)。 Wednesday 8:30-10:10 @ Room 155 [Orals/Spotlights]

## [Zero-Shot Sketch-Image Hashing](https://arxiv.org/pdf/1803.02284.pdf)


## [Relation Networks for Object Detection](https://arxiv.org/pdf/1711.11575.pdf)


## [What do Deep Networks Like to See?](https://arxiv.org/pdf/1803.08337.pdf)
把一个训练好的自编码器的编码器固定，把一个训练好的分类网络固定，用分类的损失更新自编码器里的解码器。这样就能训练自编码器让有利于分类正确的部分通过并去除输入图片里的噪声。

发现用不同分类网络训练出来的解码器图像都不一样。VGG的解码器图像和原图最接近，AlexNet和ResNet都带来了棋盘格子效应。Inception的解码器图片带有粉色调。

用分类网络训练出来的解码器有去噪作用。给图片加上均匀分布的噪声，比较直接用加噪图片分类、用加噪图片经过预训练解码器分类、加噪图片用分类训练解码器处理再分类，发现用分类训练的解码器处理噪声图片之后分类的精度对噪声最不敏感。

交换解码器的实验。比如说VGG的解码器给ResNet用这种，可以比较不同的分类网络利用的信息量大小。

## Free Supervision From Video Games
想看来着没找到pdf

## Adversarial Metric Learning
上网搜了一下只有篇[IJCAI的标题一模一样的论文](https://arxiv.org/abs/1802.03170)。等CVPR的出来了再看看

## [Decoupled Networks](https://arxiv.org/pdf/1804.08071.pdf)
作者发现CNN的特征本身是decoupled的，幅度表示intra-class variation，角度表示semantic difference。提出把这种关系显示表达：图像块$\textbf{x}$和kernel $\textbf{w}$的内积$<\textbf{x}, \textbf{w}>$可以写成$||\textbf{w}||_2 ||\textbf{x}||_2 \cos (\theta(\textbf{w}, \textbf{x}))$。根据这个把现有CNN的内积操作换成关于幅度的函数$h(||\textbf{w}||, ||\textbf{x}||)$和关于角度的函数$g(\theta(\textbf{w}, \textbf{x}))$的乘积。

看起来有意思。[作者主页](http://wyliu.com/)给了代码链接但是打不开

## [Excitation Backprop for RNNs](https://arxiv.org/pdf/1711.06778.pdf)
好像是把CNN的exitation backprop扩展到RNN了。先看看CNN的。

### [Top-down Neural Attention by Excitation Backprop](https://arxiv.org/pdf/1608.00507.pdf)
解决的问题：一个CNN的最后一个全连接层输出了某个分类结果（即某个类的得分最高），那么从第一层到最后一层这一路上都是那些神经元被激活了，导致最后的分类结果？

设$C_j$是和$a_i$相连的浅一层的所有神经元组成的集合。把P(a_j激活|a_i激活)定义为eqn.4。那么就有$P(a_j) = \sum_{i} P(a_j|a_i) P(a_i)$。

懒得写了。。

## [	Bootstrapping the Performance of Webly Supervised Semantic Segmentation]
这个也打算看的但没找到pdf

## [	Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features]
打算看的，结果没找到pdf

## [Adversarial Complementary Learning for Weakly Supervised Object Localization](https://arxiv.org/pdf/1804.06962.pdf)
对CAM和AE的改进。就是把AE变成并行的了。首先用分类器A得到class activation maps，然后把特征图里对应A的class activation maps值比较大的像素置0，输入分类器B得到互补的class activation maps，然后两个取最大值得到最终的class activation maps。

### [复习CAM (Class Activation Mapping](https://arxiv.org/pdf/1512.04150.pdf)
一个CNN最后一个卷积层输出的长H宽H通道K的特征图$S$经过平均池化，得到一个K维的向量$\textbf{s}$。用这个K维向量和一个KxC的矩阵相乘得到每个类的得分。即第c类的得分$y_c = \sum_{i=1}^K s_i W_{ic}$。注意到这个向量$\textbf{s}$是由特征图平均池化得到的。如果不对特征图池化，而是直接进行加权和$Y_c = \sum_{i=1}^K S_i W_{ic}$，就能得到第c类的activation map了。

### [STC](http://users.eecs.northwestern.edu/~xsh835/assets/pami_stc.pdf)
读这篇时发现他们的一个参考文献。。用有image level的标签的图片加上显著性检测，产生语义分割的标签。我前一阵本想这么做的，又发现已经有人这么做过了。。。

## [Image Generation from Scene Graphs](https://arxiv.org/pdf/1804.01622.pdf)
用节点表示场景里的物体，边表示物体之间的关系。首先用一个神经网络处理场景图（大概是把边和节点都one-hot编码的场景图吧），得到每个节点和边的向量。然后把这个向量输入方框回归网络和产生物体mask的网络。然后把物体的mask填到方框里，得到场景布局（即语义分割的真值那种样子）。最后用Cascaded Refinement Netword根据场景布局生成图片。

## [Compare and Contrast: Learning Prominent Visual Differences](https://arxiv.org/pdf/1804.00112.pdf)
输出并比较两个图片主要的不同。没仔细看

## [Hierarchical Novelty Detection for Visual Object Recognition](https://arxiv.org/pdf/1804.00722.pdf)
利用层次分类学，让模型对未见过的样本给出更informative的分类结果，而不是仅仅分类成没见过的类。比如训练时没见过安哥拉猫，测试时能把安哥拉猫分类成没见过的猫，而不仅仅是没见过的类。这篇文章提供了两个模型，一个top-down和一个flatten模型。

**top-down**

为“以超类为条件，子类的条件概率$P(y|x, s; \theta_s)$”建模。对于一个超类$s$，如果以它为条件的各个子类的分布和均匀分布的KL距离大于阈值$\lambda_s$，就把其中概率最大的子类作为下一个超类。然后再以这个新的超类为条件，继续计算条件概率$P(y|x, s;\theta_s)$。直到终于出现了均匀分布的条件概率、或者到了一个叶子节点。为了使得模型能在unconfident的时候给出接近均匀分布的预测，损失函数除了要最大化以正确超类$s$为条件的子类分类的概率$P(y|x,s;\theta_s)$之外，还要最小化以错误超类$\mathcal{O}(s)$为条件的子类分类的概率和均匀分布之间的KL距离。（论文里的eqn.(1)）

**flatten**: 

为每个超类增加一个子节点作为新类，比如，猫的子节点有波斯猫和暹罗猫，增加子节点之后，猫的子节点就有波斯猫、暹罗猫和新猫。训练时，对于一个类的样本，依次把它从每个层次移除作为那个层次的新类。比如说，第一阶段把波斯猫移除作为新猫，第二阶段把所有猫移除作为新动物。【这里没有看得很懂。训练时到底移除哪个类呢？从eqn.(3)看，应该是每个类都会被移除，然后对所有类取数学期望。具体到实践上如何操作呢？作者说在附录里，太复杂了懒得看了】

## [Imagine it for me: Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts](https://arxiv.org/pdf/1712.01381.pdf)
把gan应用在0样本学习。用gan根据维基百科的文字描述产生未知样本的feature、把0样本学习转化成了全监督学习。

## [Dynamic-structured Semantic Propagation Network](https://arxiv.org/pdf/1803.06067.pdf)
现有的模型没有考虑类之间的semantic hierarchy。比如，长颈鹿/斑马/马就有很多明显区别于猫/狗的特点。这篇的模型，训练时按照类的分层建立动态的计算图。

## [The Unreasonable Effectiveness of Deep Features as a Perceptual Metric](https://arxiv.org/pdf/1801.03924.pdf)
他们发现用CNN的特征计算的图片感知距离，无论用的是unsupervised, self-supervsied, supervsied的，都很符合人类的感知（与传统的L2, SSIM, PSNR相比）。

## [Learning Superpixels with Segmentation-Aware Affinity Loss](https://sites.google.com/site/wctu1009/cvpr18_superpixel)
有一丝兴趣，但没搜到全文，等有全文了再看看

## [Scale-Transferrable Object Detection](https://pan.baidu.com/s/1i6Yjvpz)
是我没看懂吗？貌似就是在densenet的feature上用了pixel-suffle上采样，还给pixel-suffle取名为scale transferrable module。。。。

## [Disentangled Person Image Generation](https://arxiv.org/pdf/1712.02621.pdf)
人图片的三个因素是姿势、前景、背景。首先，理清输入图像的这三个因素并用特征来编码【即把人图片的pose, 前景/背景mask分别映射成向量】，并用这编码的特征重建图像。把高斯噪声映射到这种特征，产生新图片。【似乎就是vae和gan的结合，特别针对人图片的】

## [ICLR18有个用gan做one-shot learning的](https://openreview.net/pdf?id=S1Auv-WRZ)
懒得看了，下次再看

## [one-class classification](https://arxiv.org/pdf/1802.09088.pdf)
用GAN做D区分R(x)和x，然后D作为那个新样本检测器

## [一个新的vqa数据集](https://arxiv.org/abs/1801.08163)

## [Multi-Agent Diverse Generative Adversarial Networks](https://arxiv.org/abs/1704.02906)

## [Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning](https://arxiv.org/abs/1802.09129)
上次组会有人讲了，懒得看了

## [self-contradiction什么的](https://arxiv.org/abs/1604.05132)

## [Learning to Adapt Structured Output Space for Semantic Segmentation](https://arxiv.org/abs/1802.10349)。
用gan做domian adaptationa，看起来有点意思，[gta5dataset](https://download.visinf.tu-darmstadt.de/data/from_games/index.html)

用游戏生成的训练数据(source domian)训练分割模型。因为游戏数据和真实数据的target是相似的，所以在真实数据上测试时用游戏数据的标签作domain adaptation。具体：用D分类预测的标签和游戏数据的标签。

## [semi parametric](http://vladlen.info/papers/SIMS.pdf)
此篇引用了我那个只得了70分的本科毕设:)

输入语义分割，输出图片怎么做：对于语义分割里的每个区域，在训练集里检索一个最类似的图片区域，经过训练仿射变换和裁剪，拼凑在一起。（仿射变换和裁剪的模型（模型1）是预训练的）然后再去除拼接的边缘。把拼接的结果输入一个模型，训练这个模型（模型2）弥补边缘。用生成图片和真实图片的感知距离（VGGfeature的L1）距离训练模型2。

## [Visual Dialog with Discriminative Question Generation and Answering](https://arxiv.org/pdf/1803.11186.pdf)

## [vqa memory argumented](https://arxiv.org/abs/1707.04968)

## [single shot finement](https://arxiv.org/abs/1711.06897)

## [dynamic zoom in 大图片](https://arxiv.org/abs/1711.05187)

## [vqa和vqg的对偶](http://cvboy.com/pdf/publications/cvpr2018_iqan.pdf)
本来打算当做开题答辩的思路，突然发现已经被大佬们做过了，，，
