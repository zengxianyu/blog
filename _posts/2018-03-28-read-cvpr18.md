---
layout: post
title: 阅读cvpr18的论文
---
## [Hierarchical Novelty Detection for Visual Object Recognition](https://arxiv.org/pdf/1804.00722.pdf)
利用层次分类学，让模型对未见过的样本给出更informative的分类结果，而不是仅仅分类成没见过的类。比如训练时没见过安哥拉猫，测试时能把安哥拉猫分类成没见过的猫，而不仅仅是没见过的类。这篇文章提供了两个模型，一个top-down和一个flatten模型。

__top-down__: 为“以超类为条件，子类的条件概率——$P(y|x, s; \theta_s)$”建模。对于一个超类$s$，如果以它为条件的各个子类的分布和均匀分布的KL距离大于阈值$\lambda_s$，就把其中概率最大的子类作为下一个超类。然后再以这个新的超类为条件，继续计算条件概率$P(y|x, s;\theta_s)$。直到终于出现了均匀分布的条件概率、或者到了一个叶子节点。为了使得模型能在unconfident的时候给出接近均匀分布的预测，损失函数除了要最大化以正确超类$s$为条件的子类分类的概率$P(y|x,s;\theta_s)$之外，还要最小化以错误超类$\mathcal{O}(s)$为条件的子类分类的概率和均匀分布之间的KL距离。（论文里的eqn.(1)）

__flatten__: 为每个超类增加一个子节点作为新类，比如，猫的子节点有波斯猫和暹罗猫，增加子节点之后，猫的子节点就有波斯猫、暹罗猫和新猫。训练时，对于一个类的样本，依次把它从每个层次移除作为那个层次的新类。比如说，第一阶段把波斯猫移除作为新猫，第二阶段把所有猫移除作为新动物。【这里没有看得很懂。训练时到底移除哪个类呢？从eqn.(3)看，应该是每个类都会被移除，然后对所有类取数学期望。具体到实践上如何操作呢？作者说在附录里，太复杂了懒得看了】

## [Imagine it for me: Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts](https://arxiv.org/pdf/1712.01381.pdf)
把gan应用在0样本学习。用gan根据维基百科的文字描述产生未知样本的feature、把0样本学习转化成了全监督学习。

## [Dynamic-structured Semantic Propagation Network](https://arxiv.org/pdf/1803.06067.pdf)
现有的模型没有考虑类之间的semantic hierarchy。比如，长颈鹿/斑马/马就有很多明显区别于猫/狗的特点。这篇的模型，训练时按照类的分层建立动态的计算图。

## [The Unreasonable Effectiveness of Deep Features as a Perceptual Metric](https://arxiv.org/pdf/1801.03924.pdf)
他们发现用CNN的特征计算的图片感知距离，无论用的是unsupervised, self-supervsied, supervsied的，都很符合人类的感知（与传统的L2, SSIM, PSNR相比）。

## [Learning Superpixels with Segmentation-Aware Affinity Loss](https://sites.google.com/site/wctu1009/cvpr18_superpixel)
有一丝兴趣，但没搜到全文，等有全文了再看看

## [Scale-Transferrable Object Detection](https://pan.baidu.com/s/1i6Yjvpz)
是我没看懂吗？貌似就是在densenet的feature上用了pixel-suffle上采样，还给pixel-suffle取名为scale transferrable module。。。。

## [Disentangled Person Image Generation](https://arxiv.org/pdf/1712.02621.pdf)
人图片的三个因素是姿势、前景、背景。首先，理清输入图像的这三个因素并用特征来编码【即把人图片的pose, 前景/背景mask分别映射成向量】，并用这编码的特征重建图像。把高斯噪声映射到这种特征，产生新图片。【似乎就是vae和gan的结合，特别针对人图片的】

## [ICLR18有个用gan做one-shot learning的](https://openreview.net/pdf?id=S1Auv-WRZ)
懒得看了，下次再看

## [one-class classification](https://arxiv.org/pdf/1802.09088.pdf)
用GAN做D区分R(x)和x，然后D作为那个新样本检测器

## [一个新的vqa数据集](https://arxiv.org/abs/1801.08163)

## [Multi-Agent Diverse Generative Adversarial Networks](https://arxiv.org/abs/1704.02906)

## [Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning](https://arxiv.org/abs/1802.09129)
上次组会有人讲了，懒得看了

## [self-contradiction什么的](https://arxiv.org/abs/1604.05132)

## [Learning to Adapt Structured Output Space for Semantic Segmentation](https://arxiv.org/abs/1802.10349)。
用gan做domian adaptationa，看起来有点意思，[gta5dataset](https://download.visinf.tu-darmstadt.de/data/from_games/index.html)

用游戏生成的训练数据(source domian)训练分割模型。因为游戏数据和真实数据的target是相似的，所以在真实数据上测试时用游戏数据的标签作domain adaptation。具体：用D分类预测的标签和游戏数据的标签。

## [semi parametric](http://vladlen.info/papers/SIMS.pdf)
此篇引用了我那个只得了70分的本科毕设:)

输入语义分割，输出图片怎么做：对于语义分割里的每个区域，在训练集里检索一个最类似的图片区域，经过训练仿射变换和裁剪，拼凑在一起。（仿射变换和裁剪的模型（模型1）是预训练的）然后再去除拼接的边缘。把拼接的结果输入一个模型，训练这个模型（模型2）弥补边缘。用生成图片和真实图片的感知距离（VGGfeature的L1）距离训练模型2。

## [Visual Dialog with Discriminative Question Generation and Answering](https://arxiv.org/pdf/1803.11186.pdf)

## [vqa memory argumented](https://arxiv.org/abs/1707.04968)

## [single shot finement](https://arxiv.org/abs/1711.06897)

## [dynamic zoom in 大图片](https://arxiv.org/abs/1711.05187)

## [vqa和vqg的对偶](http://cvboy.com/pdf/publications/cvpr2018_iqan.pdf)
本来打算当做开题答辩的思路，突然发现已经被大佬们做过了，，，
