
## [Revisiting Video Saliency: A Large-scale Benchmark and a New Model](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Revisiting_Video_Saliency_CVPR_2018_paper.pdf)
* 现有数据集尺寸小、范围窄。没有单独测试集的服务器
* 有单独测试集的服务器。1K个视频，超过600K frames，每frame有来自17个人的标注。每个视频都有一个类别标签(1/150)和一些属性标签（内容移动、相机移动、光照、出现的物体数量、出现的人的数量等）600 train/100 val/ 300 test
* 提出了个基于CNN-LSTM模型。

convLSTM: 输入、输出、hidden state、cell state都是feature maps。用卷积层处理输入信号和hidden state

VGG的最后一个卷积层的输出预测一个attention map，放大两倍，记为M。VGG的去掉最后两个池化层所产生的特征图记为$\mathcal{X}$，把序列的$\mathcal{X} + M o \mathcal{X}$输入convLSTM产生预测结果。

训练：交替地计算M和真值的、预测结果和真值的损失

## [Going from Image to Video Saliency: Augmenting Image Salience with Dynamic Attentional Push](http://openaccess.thecvf.com/content_cvpr_2018/papers/Gorji_Going_From_Image_CVPR_2018_paper.pdf)
attentional push: 被拍摄的人的attention对看这幅图片或视频的人的attention的影响。扩展到视频，就还有摄影师的attention对观众的attention的影响。

e.g., Figure 1

[21]研究了图片里的人物的注视方向对观察者的attention的影响，称之为attentional push effect。这篇文章考虑视频里的attentional push effect。从图片扩展到视频之后，除了被拍摄者的注视点，要考虑的成分还有被注视的人物出镜时加入center bias priors、场景突然改变时加入center bias priors。

### Augmenting Image Salience
现有的图片saliency方法产生的结果输入convLSTM，得到这一分支的预测结果

### 被拍摄者的注视点
被拍摄者的头部、被拍摄者的头部在整个图片里的位置图，输入CNN，输出被拍摄者的注视点位置图。位置图输入convLSTM，得到这一分支的预测结果

LSTM前面的部分用[21]提出的数据集（包含头部框和被拍摄者的注视点的真值）预训练

### 被拍摄者出境和场景突变
用现有的方法判断是否发生了这两种情况。如果发生了，就把几个协方差不同的二维高斯函数输入convLSTM，否则把全0输入convLSTM

### 最后输出
把四个convLSTM的预测结果串联，经过卷积层产生一个最后的预测结果。

### 训练
训练时，先用真值分别训练这4个convLSTM。最后在训练整个体系。

训练被拍摄者注视点分支时，用人工标注的被拍摄者头部作为输入。测试时，用现有的脸部检测方法产生的头部作为输入。

## [Flow Guided Recurrent Neural Encoder for Video Salient Object Detection](http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Flow_Guided_Recurrent_CVPR_2018_paper.pdf)
* 传统方法使用了光流估计的结果，但是是用在off-the-shelf的后处理
* 现有的基于CNN的方法通常把几个相邻帧叠在一起输入CNN，没有考虑物体的长期变化。

作者：把光流估计作为整个体系的一部分一起训练更新

### 做法
对于每一帧，用FlowNet估计前面几帧到这一帧的光流，得到一个光流序列。输入convLSTM。根据光流序列从这一帧的特征图恢复前面几帧的特征图。把特征图序列输入另一个convLSTM，再把这个LSTM的最后一个hidden state的特征图输入像素分类器，输出预测结果。
