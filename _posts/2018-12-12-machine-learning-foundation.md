---
layout: post
title: 机器学习基础
---
模型由假设函数（所有可能的函数）和算法（从假设函数里选个g approximate target f）组成。

## 例模型
首先介绍了感知机模型和两个算法：用于线性可分数据集的感知机学习模型PLA和用于非线性可分数据集的pocket

假设函数是一个分类平面，平面这边的点函数值为1，另一边为-1：

$$h(x) = sign(\sum_{i=1}^d w_i x_i - \theta)$$

把$\theta$作为$w_0$，可以写成内积形式

$$h(x) = sign(w^T x), x_0=1$$

解释：判断特征的加权和是否大于阈值。

### PLA
从所有可能的平面里选一个使得正确分类所有训练数据

1. 从任意一个平面$w_0$开始
2. 每次迭代t，找一个当前的平面$w_t$分类错误的$x_{n(t)}, y_{n(t)}$，即$y_{n(t)} \ne w_t^Tx_{n(t)}$
3. 修正$w_t$。$w_{t+1} \leftarrow w_t + y_{n(t)}x_{n(t)}$
4. 直到所有点都正确分类了

每次更新都往能够正确分类一个错误的数据的方向移动了一点。

收敛性证明：原理是求$w_T$的上界和下界，然后根据上界大于下界得出关于T的不等式

设$w_0=0$，$w_f$是正确分类的平面，即$\forall n, y_{n} w_f^T x_{n}>0$。

1. 根据$w_{t+1} = w_t + y_{n(t)}x_{n(t)}$，得到$||w_{t+1}||^2\le||w_t||^2 + ||x_{n(t)}||^2$，所以$||w_T||^2\le TR^2$，其中$R=\max_n ||x_n||$
2. $w_{t+1} = w_t + y_{n(t)}x_{n(t)}$等号两边同时左乘$w_f^T$得$w_f^Tw_{t+1}=w_f^Tw_t + y_{n(t)} w_f^Tx_{n(t)}$，所以$w_f^Tw_T > T\gamma$，其中$\gamma=\max_n y_n w_f^T x_n$。所以$||w_f||\cdot||w_T||\ge w_f^Tw_T>T\gamma$，即$||w_T||>\frac{T\gamma}{||w_f||}$。（从这里有个直观感受，随T增大，$w_T$和$w_f$的内积增大，说明平面和正确分类的平面越来越接近）
3. 根据1和2，$\frac{T^2\gamma^2}{||w_f||^2}<TR^2$，即$T<\frac{R^2||w_f||^2}{\gamma^2}$

### Pocket
1. 随机$w_0$
2. 计算$w_{t+1} = w_t + y_{n(t)}x_{n(t)}$
3. 如果$w_{t+1}$错误分类的数据更少就更新，否则不更新


